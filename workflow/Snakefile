## Snakefile for calculating mtDNA copy number

import os.path
from os import path
import pandas as pd
RWD = os.getcwd()

shell.prefix('module load R/3.6.3 samtools singularity freebayes/1.3.2 vcflib bcftools tabix bedtools; ')

configfile: "config/config.yaml"
SAMPLES = pd.read_csv('data/SampleInfo.csv', index_col='SampleID')
# SAMPLES = pd.read_csv("sandbox/SampleInfo_test.csv", index_col='SampleID')
# SAMPLES = pd.read_csv('data/SampleInfo_msbb.csv', index_col='SampleID')

rule all:
    input:
        expand("data/mity/{SAMPLE_IDS}.chrM.mity.vcf.gz", SAMPLE_IDS = SAMPLES.index.tolist()),
        expand("data/haplogrep/{SAMPLE_IDS}.haplogrep.txt", SAMPLE_IDS  = SAMPLES.index.tolist()),
        "data/haplogrep/haplogrep_individualAll.txt",
        expand("data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf.gz", SAMPLE_IDS = SAMPLES.index.tolist()),
        'data/freebayes/joint/joint_chrM.snv.vcf',
        expand("data/mosdepth/{SAMPLE_IDS}.mosdepth.summary.txt", SAMPLE_IDS = SAMPLES.index.tolist()),
        "data/mosdepth/mosdepth_mtDNAcn_All.txt",
        "data/haplogrep/haplogrep_jointAll.txt",
        # Mitoverse
        "data/mitoverse/results/mutserve_all.vcf.gz",
        'data/mitoverse/results/haplocheck_res.txt',
        'data/mitoverse/results/haplocheck_res.raw.txt',
        'data/mitoverse/results/haplocheck_res.html',
        'data/mitoverse/results/haplogroups/haplogroups.txt',
        "data/mitoverse/results/mutserve_anovarAll.txt"

## Extract and copy across mitochondrial genome
rule copy:
    input:
        bam = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['wgs'],
        bai = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['index']
    output:
        bam = temp('temp/{SAMPLE_IDS}.chrM.bam'),
        bai = temp('temp/{SAMPLE_IDS}.chrM.bam.bai')
    # conda: 'envs/samtools.yaml'
    run:
        ## For samples aligned to hg37 extract chromsome MT
        if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 37:
            shell('samtools view -b {input.bam} MT > {output.bam}; samtools index -b {output.bam}')
            print("Using hg37 as reference build")
        ## For samples aligned to hg38 extract chromsome chrM
        elif SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 38:
            shell("samtools view -h {input.bam} chrM | sed 's/chrM/MT/g' | samtools view -b -o {output.bam}; samtools index -b {output.bam}")
            print("Lifting overfrom hg38 to hg37")
            # shell('samtools view -b {input.bam} chrM > {output.bam}; samtools index -b {output.bam}')
            # print("Using hg38 as reference build")
        else:
            print(SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] + ': Build Not Recognised')

# rule fastMitoCalc:
#     input:
#         bam = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['wgs'],
#         bai = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['index']
#     output: "data/mitocalc/{SAMPLE_IDS}MTData.txt"
#     params:
#         wd = 'data/mitocalc'
#     run:
#         ## For samples aligned to hg37 use default chromosome prefix (none) and mtname (MT)
#         if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 37:
#             shell('perl src/fastMitoCalc/fastMitoCalc.pl -f {input.bam} -w {params.wd} -p src/fastMitoCalc/BaseCoverage')
#             print("Using hg37 as reference build")
#         ## For samples aligned to hg38 specify chromosome prefix (chr) and mtname (chrM)
#         elif SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 38:
#             shell('perl src/fastMitoCalc/fastMitoCalc.pl -f {input.bam} -e chr -m chrM -w {params.wd} -p src/fastMitoCalc/BaseCoverage')
#             print("Using hg38 as reference build")
#         else:
#             print(SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] + ': Build Not Recognised')


rule mosdepth:
    input:
        bam = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['wgs'],
        bai = lambda wildcards: SAMPLES.loc[wildcards.SAMPLE_IDS]['index'],
        fa = lambda wildcards: 'raw/reference/Homo_sapiens_assembly19.fasta' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 37 else ('raw/reference/Homo_sapiens_assembly38.fasta' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 38 else None)
    output: "data/mosdepth/{SAMPLE_IDS}.mosdepth.summary.txt"
    params:
        wd = 'data/mosdepth/{SAMPLE_IDS}'
    # conda: 'envs/mosdepth.yaml'
    run:
        ## For samples in bam format
        if SAMPLES.loc[wildcards.SAMPLE_IDS]['ext'] == 'bam':
            shell('./src/mosdepth -n -t 4 {params.wd} {input.bam}')
            print("Calculating coverage from .bam file")
        ## For samples in cram format
        elif SAMPLES.loc[wildcards.SAMPLE_IDS]['ext'] == 'cram':
            shell('./src/mosdepth -n  -t 4 -f {input.fa} {params.wd} {input.bam}')
            print("Calculating coverage from .cram file")
        else:
            print(SAMPLES.loc[wildcards.SAMPLE_IDS]['ext'] + ': File Format not recongised')

rule mtDNAcn_mosdepth:
    input:
        mosdepth = "data/mosdepth/{SAMPLE_IDS}.mosdepth.summary.txt"
    output:
        mtdnacn = "data/mosdepth/{SAMPLE_IDS}.mosdepth.mtDNAcn.txt"
    params:
        id = '{SAMPLE_IDS}'
    conda: 'envs/r.yaml'
    script: "../workflow/scripts/mtDNAcn_mosdepth.R"

def mtdnacn_input(wildcards):
    return expand("data/mosdepth/{SAMPLE_IDS}.mosdepth.mtDNAcn.txt", SAMPLE_IDS = SAMPLES.index.tolist())

rule merge_mtdnacn:
    input: mtdnacn_input,
    output: "data/mosdepth/mosdepth_mtDNAcn_All.txt"
    shell: "awk 'FNR==1 && NR!=1{{next;}}{{print}}' {input} > {output}"

# singularity build mity.sif docker://drmjc/mity
rule mity:
    input:
        bam = 'temp/{SAMPLE_IDS}.chrM.bam',
        bai = 'temp/{SAMPLE_IDS}.chrM.bam.bai'
    output:
        vcf = "data/mity/{SAMPLE_IDS}.chrM.mity.vcf.gz",
        tbi = "data/mity/{SAMPLE_IDS}.chrM.mity.vcf.gz.tbi",
    params:
        # ref = lambda wildcards: 'hs37d5' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 37 else ('hg38' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 38 else None),
        ref = "hs37d5",
        wd = 'data/mity'
    container: "docker://drmjc/mity:0.1.3"
    shell:
        'mity call --reference {params.ref} --out-folder-path {params.wd} --normalise {input.bam}'

rule freebayes:
    input:
        bam = 'temp/{SAMPLE_IDS}.chrM.bam',
        bai = 'temp/{SAMPLE_IDS}.chrM.bam.bai',
        # fa = lambda wildcards: 'raw/reference/hs37d5.MT.fa' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 37 else ('raw/reference/hg38.chrM.fa' if SAMPLES.loc[wildcards.SAMPLE_IDS]['hg'] == 38 else None)
        fa = 'raw/reference/hs37d5.MT.fa'
    output:
        vcf = "data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf",
    conda: 'envs/freebayes.yaml'
    shell:
        """
        freebayes -f {input.fa} -b {input.bam} \
        --min-mapping-quality 30 \
        --min-base-quality 24 \
        --min-alternate-fraction 0.6 \
        --min-alternate-count 4 \
        --ploidy 1 \
        | vcffilter -f "QUAL > 20" > {output.vcf}
        """

rule bgzip:
    input: vcf = "data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf"
    output:
        bcf = "data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf.gz",
        tbi = "data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf.gz.tbi"
    shell:
        'bgzip {input.vcf}; tabix {output.bcf}'


def vcf_input(wildcards):
    return expand("data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf.gz", SAMPLE_IDS = SAMPLES.index.tolist())

def bam_input(wildcards):
    return expand('temp/{SAMPLE_IDS}.chrM.bam', SAMPLE_IDS = SAMPLES.index.tolist())

rule list_files:
    input:
        vcfs = vcf_input,
        bams = bam_input,
    output:
        file_list = 'data/VcfBamPaths.txt'
    run:
        filepaths = input.vcfs + input.bams
        with open(output.file_list, "w") as filehandle:
          for listitem in filepaths:
            filehandle.write("%s\n" % listitem)

rule vcf_merge:
    input:
        files = 'data/VcfBamPaths.txt',
        fa = 'raw/reference/hs37d5.MT.fa'
    output:
        vcf = 'data/freebayes/joint_chrM.fb.vcf.gz'
    conda: 'envs/bcbio.yaml'
    shell: './src/bcbio-variation-recall square -m freebayes -c 16 {output.vcf} {input.fa} {input.files}'

## Clean Freebayes Joint Called VCF files
### normalize indels; decompose multiallelics; intergrate multiallelics; calcualte AF etc; extract SNVs
rule vcf_clean:
    input:
        vcf = 'data/freebayes/joint_chrM.fb.vcf.gz',
        fa = "raw/reference/hs37d5.MT.fa"
    output:
        vcf = 'data/freebayes/joint/joint_chrM.snv.vcf',
    params:
        stem = "data/freebayes/joint/joint_chrM"
    conda: "envs/bcftools.yaml"
    shell:
        """
        ml vt
        bcftools norm -f {input.fa} {input.vcf} | \
        bcftools norm -m -any  | \
        bcftools norm -m +both | \
        bcftools plugin fill-tags -Ov -o {params.stem}.normalized.vcf;
        vt view {params.stem}.normalized.vcf -f "VTYPE==SNP&&N_ALLELE==2" -o {params.stem}.snv.vcf;
        """

rule joint_haplogrep:
    input:
        vcf = 'data/freebayes/joint/joint_chrM.snv.vcf',
    output:
        "data/haplogrep/haplogrep_jointAll.txt",
    shell:
        'java -jar src/haplogrep-2.1.25.jar --in {input.vcf} --format vcf --out {output}'

rule indiv_haplogrep:
    input:
#        vcf = "data/mity/{SAMPLE_IDS}.chrM.mity.vcf.gz",
        vcf = "data/freebayes/{SAMPLE_IDS}.chrM.fb.vcf.gz",
    output:
        out = "data/haplogrep/{SAMPLE_IDS}.haplogrep.txt",
    shell:
        'java -jar src/haplogrep-2.1.25.jar --in {input.vcf} --format vcf --out {output.out}'

## Meger individual called haplogrep files
def haplogrep_input(wildcards):
    return expand("data/haplogrep/{SAMPLE_IDS}.haplogrep.txt", SAMPLE_IDS = SAMPLES.index.tolist())

rule merge_haplorep:
    input:
        haplogrep = haplogrep_input,
    output:
        "data/haplogrep/haplogrep_individualAll.txt"
    shell:
        "awk 'FNR==1 && NR!=1{{next;}}{{print}}' {input.haplogrep} > {output}"

## Mutserve
rule mutserve_call:
    input:
        bam = 'temp/{SAMPLE_IDS}.chrM.bam',
        bai = 'temp/{SAMPLE_IDS}.chrM.bam.bai',
        ref = "src/mutserve/rCRS.fasta"
    output:
        vcf = "data/mitoverse/mutserve/{SAMPLE_IDS}.vcf.gz",
        txt = "data/mitoverse/mutserve/{SAMPLE_IDS}.txt"
    shell: "./src/mutserve/mutserve call --reference {input.ref} --output {output.vcf} --threads 4 {input.bam}"

rule mutserve_index:
    input: rules.mutserve_call.output.vcf
    output: "data/mitoverse/mutserve/{SAMPLE_IDS}.vcf.gz.tbi"
    shell: "bcftools index -t {input}"

rule muterve_annotate:
    input:
        txt = "data/mitoverse/mutserve/{SAMPLE_IDS}.txt",
        annotation = "src/mutserve/rCRS_annotation_2020-08-20.txt"
    output: "data/mitoverse/mutserve/{SAMPLE_IDS}_anovar.txt"
    shell: "./src/mutserve/mutserve annotate --input {input.txt} --annotation {input.annotation} --output {output}"

def muterve_annotate_input(wildcards):
    return expand("data/mitoverse/mutserve/{SAMPLE_IDS}_anovar.txt", SAMPLE_IDS = SAMPLES.index.tolist())
def merge_vcf_input(wildcards):
    return expand("data/mitoverse/mutserve/{SAMPLE_IDS}.vcf.gz", SAMPLE_IDS = SAMPLES.index.tolist())
def merge_index_input(wildcards):
    return expand("data/mitoverse/mutserve/{SAMPLE_IDS}.vcf.gz.tbi", SAMPLE_IDS = SAMPLES.index.tolist())

rule merge_muterve_annotate:
    input: muterve_annotate_input,
    output: "data/mitoverse/results/mutserve_anovarAll.txt"
    shell:
        "awk 'FNR==1 && NR!=1{{next;}}{{print}}' {input} > {output}"

rule merge_vcfs:
    input:
        vcf = merge_vcf_input,
        index = merge_index_input
    output: "data/mitoverse/results/mutserve_all.vcf.gz"
    shell: "bcftools merge --missing-to-ref -Oz -o {output} {input.vcf}"

rule haplocheck_hg:
    input: "data/mitoverse/results/mutserve_all.vcf.gz"
    output:
        'data/mitoverse/results/contamination/contamination.txt',
        'data/mitoverse/results/haplogroups/haplogroups.txt',
        'data/mitoverse/results/report/report.html'
    params:
        out = RWD+'/data/mitoverse/results',
        input = RWD+"/data/mitoverse/results/mutserve_all.vcf.gz"
    shell: "./src/haplocheck/cloudgene run haplocheck@1.3.2 --files {params.input} --format vcf --output {params.out} --threads 1"

rule haplocheck_raw:
    input: rules.merge_vcfs.output
    output:
        'data/mitoverse/results/haplocheck_res.txt',
        'data/mitoverse/results/haplocheck_res.raw.txt',
        'data/mitoverse/results/haplocheck_res.html'
    params:
        out = 'data/mitoverse/results/haplocheck_res.txt'
    shell: "./src/haplocheck/haplocheck --out {params.out} {input} --raw"
